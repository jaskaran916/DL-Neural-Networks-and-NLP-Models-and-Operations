{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Impact of network architecture on information flow and learning capabilities**","metadata":{"id":"Pesv5LtCMUXl"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom sklearn.metrics import mutual_info_score\nimport os\n\ndef estimate_mutual_info(X, Y, n_bins=30):\n    bins_x = np.linspace(np.min(X), np.max(X), n_bins + 1)\n    x_b = np.digitize(X, bins_x) - 1\n    y_b = Y if Y.dtype.kind in 'i' else np.digitize(Y, bins_x) - 1\n    return mutual_info_score(x_b, y_b)\n\nclass MLP(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        layers = []\n        for i in range(len(dims) - 2):\n            layers.append(nn.Linear(dims[i], dims[i+1]))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(dims[-2], dims[-1]))\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        activations = []\n        out = x\n        for layer in self.network:\n            out = layer(out)\n            if isinstance(layer, nn.ReLU):\n                activations.append(out.detach())\n        return out, activations\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, channels, 5, padding=2)\n        self.pool1 = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(channels, channels*2, 5, padding=2)\n        self.pool2 = nn.MaxPool2d(2)\n        self.fc = nn.Linear((channels*2)*7*7, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        activations = []\n        out = self.relu(self.conv1(x))\n        activations.append(out.view(out.size(0), -1).detach())\n        out = self.pool1(out)\n        out = self.relu(self.conv2(out))\n        activations.append(out.view(out.size(0), -1).detach())\n        out = self.pool2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out, activations\n\ndef run_arch_experiment(arch_name, model, train_loader, test_loader, device, epochs=5):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    mi_x_t = {}\n    mi_t_y = {}\n    layer_count = None\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs, _ = model(imgs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        all_acts = []\n        all_x = []\n        all_y = []\n        with torch.no_grad():\n            for imgs, labels in test_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                out, acts = model(imgs)\n                if not all_acts:\n                    all_acts = [[] for _ in acts]\n                    layer_count = len(acts)\n                for i, a in enumerate(acts):\n                    all_acts[i].append(a.cpu().numpy())\n                all_x.append(imgs.view(imgs.size(0), -1).cpu().numpy())\n                all_y.append(labels.cpu().numpy())\n\n        all_x = np.concatenate(all_x, axis=0)\n        all_y = np.concatenate(all_y, axis=0)\n\n        for i in range(layer_count):\n            if i not in mi_x_t:\n                mi_x_t[i] = []\n                mi_t_y[i] = []\n            layer_acts = np.concatenate(all_acts[i], axis=0)\n            n_neurons = min(layer_acts.shape[1], 50)\n            mi_x_vals = []\n            mi_y_vals = []\n            for n in range(n_neurons):\n                mi_x_vals.append(estimate_mutual_info(all_x[:, n], layer_acts[:, n]))\n                mi_y_vals.append(estimate_mutual_info(layer_acts[:, n], all_y))\n            mi_x_t[i].append(np.mean(mi_x_vals))\n            mi_t_y[i].append(np.mean(mi_y_vals))\n        print(f\"{arch_name} Epoch {epoch} complete\")\n\n    return mi_x_t, mi_t_y\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.ToTensor()\n    train_ds = datasets.MNIST('data', train=True, download=True, transform=transform)\n    test_ds = datasets.MNIST('data', train=False, download=True, transform=transform)\n    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n    architectures = {\n        'MLP_1layer': MLP([28*28, 128, 10]),\n        'MLP_3layer': MLP([28*28, 256, 128, 64, 10]),\n        'MLP_5layer': MLP([28*28, 512, 256, 128, 64, 10]),\n        'SimpleCNN': SimpleCNN(16)\n    }\n\n    results = {}\n    for name, model in architectures.items():\n        print(f\"Running {name}\")\n        mi_x_t, mi_t_y = run_arch_experiment(name, model, train_loader, test_loader, device)\n        results[name] = {'I(X;T)': mi_x_t, 'I(T;Y)': mi_t_y}\n\n    os.makedirs('results', exist_ok=True)\n    torch.save(results, 'results/arch_info_flow.pth')\n    print(\"Experiments done. Results saved to results/arch_info_flow.pth\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y99JthtVJ6mI","outputId":"9a3daa30-eaae-4328-c981-d58d00e022d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running MLP_1layer\n","MLP_1layer Epoch 1 complete\n","MLP_1layer Epoch 2 complete\n","MLP_1layer Epoch 3 complete\n","MLP_1layer Epoch 4 complete\n","MLP_1layer Epoch 5 complete\n","Running MLP_3layer\n","MLP_3layer Epoch 1 complete\n","MLP_3layer Epoch 2 complete\n","MLP_3layer Epoch 3 complete\n","MLP_3layer Epoch 4 complete\n","MLP_3layer Epoch 5 complete\n","Running MLP_5layer\n","MLP_5layer Epoch 1 complete\n","MLP_5layer Epoch 2 complete\n","MLP_5layer Epoch 3 complete\n","MLP_5layer Epoch 4 complete\n","MLP_5layer Epoch 5 complete\n","Running SimpleCNN\n","SimpleCNN Epoch 1 complete\n","SimpleCNN Epoch 2 complete\n","SimpleCNN Epoch 3 complete\n","SimpleCNN Epoch 4 complete\n","SimpleCNN Epoch 5 complete\n","Experiments done. Results saved to results/arch_info_flow.pth\n"]}],"execution_count":2}]}