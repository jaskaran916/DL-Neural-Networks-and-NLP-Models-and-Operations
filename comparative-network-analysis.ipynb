{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaskarandhillon1609/comparative-network-analysis?scriptVersionId=235043885\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T14:21:02.40151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CNN FOR MNIST**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Visualization of Contributions**","metadata":{}},{"cell_type":"markdown","source":"**1. Feature Maps**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndef visualize_feature_maps(model, image, layer_name=\"conv1\", save_path=\"feature_maps.png\"):\n    model.eval()\n    with torch.no_grad():\n        x = image.unsqueeze(0)  # add batch dimension\n\n        # Pass through the first conv layer + activation + pooling\n        if layer_name == \"conv1\":\n            x = F.relu(F.max_pool2d(model.conv1(x), 2))\n        elif layer_name == \"conv2\":\n            x = F.relu(F.max_pool2d(model.conv1(x), 2))\n            x = F.relu(F.max_pool2d(model.conv2(x), 2))\n        else:\n            raise ValueError(\"Only 'conv1' and 'conv2' are supported\")\n\n        feature_maps = x.squeeze(0)  # remove batch dimension\n\n        # Plot the feature maps\n        num_filters = feature_maps.shape[0]\n        fig, axes = plt.subplots(1, num_filters, figsize=(15, 3))\n        for i in range(num_filters):\n            axes[i].imshow(feature_maps[i].cpu().numpy(), cmap='gray')\n            axes[i].axis('off')\n            axes[i].set_title(f'Filter {i+1}')\n        plt.tight_layout()\n        plt.savefig(save_path)\n        plt.close()\n        print(f\"Feature maps saved to {save_path}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. PyTorch**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# Simple CNN with minimal filters for visualization\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 5, kernel_size=3)  # Only 5 filters\n        self.conv2 = nn.Conv2d(5, 10, kernel_size=3)\n        self.fc1 = nn.Linear(160, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 160)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Simulated input (like a 28x28 grayscale MNIST image)\ndummy_input = torch.randn(1, 1, 28, 28)\n\n# Initialize and run the model\nmodel = SimpleCNN()\nmodel.eval()\n\nwith torch.no_grad():\n    conv1_output = F.relu(F.max_pool2d(model.conv1(dummy_input), 2))\n\n# Plot and save feature maps\nfig, axes = plt.subplots(1, 5, figsize=(15, 3))\nfor i in range(5):\n    axes[i].imshow(conv1_output[0, i].detach().numpy(), cmap='gray')\n    axes[i].axis('off')\n    axes[i].set_title(f'Filter {i+1}')\n\nplt.tight_layout()\nplt.savefig(\"conv1_feature_maps.png\")\nprint(\"Saved: conv1_feature_maps.png\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. SHAP (for fc layers)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define CNN\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 5, kernel_size=3)\n        self.conv2 = nn.Conv2d(5, 10, kernel_size=3)\n        self.flattened_size = None  # We'll figure this out dynamically\n        self.fc1 = None\n        self.fc2 = None\n\n    def _initialize_fc_layers(self, x):\n        # Called once to set fc1 and fc2 based on the actual shape of x\n        self.flattened_size = x.view(x.size(0), -1).shape[1]\n        self.fc1 = nn.Linear(self.flattened_size, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        if self.fc1 is None or self.fc2 is None:\n            self._initialize_fc_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten dynamically\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Load MNIST\ntransform = transforms.Compose([transforms.ToTensor()])\nmnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\nloader = DataLoader(mnist, batch_size=1, shuffle=True)\n\n# Prepare background and test sets\nbackground_images = torch.cat([mnist[i][0] for i in range(100)], dim=0).reshape(100, 1, 28, 28)\ntest_images = torch.cat([mnist[i][0] for i in range(5)], dim=0).reshape(5, 1, 28, 28)\n\n# Initialize model\nmodel = SimpleCNN()\nmodel.eval()\n\n# Use SHAP\nexplainer = shap.DeepExplainer(model, background_images)\ntest_images = test_images[:5]  # Reduce batch size to 5 for testing\nshap_values = explainer.shap_values(test_images)\n\n# Visualize SHAP values\nshap.image_plot(shap_values, test_images.numpy())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Ablation Study**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Define the CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 5, kernel_size=3)\n        self.conv2 = nn.Conv2d(5, 10, kernel_size=3)\n        self.fc1 = nn.Linear(160, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 160)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Load MNIST dataset\ntransform = transforms.ToTensor()\nmnist = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\nimage, label = mnist[0]  # Get one sample image\n\n# Initialize and evaluate the original model\noriginal_model = SimpleCNN()\noriginal_model.eval()\n\n# Forward pass on original model\nwith torch.no_grad():\n    original_output = original_model(image.unsqueeze(0))\n    original_prediction = original_output.argmax(dim=1).item()\n\n# Clone and ablate (replace fc1 with identity)\nablated_model = SimpleCNN()\nablated_model.load_state_dict(original_model.state_dict())  # Copy weights\nablated_model.fc1 = nn.Identity()\nablated_model.fc2 = nn.Linear(160, 10)  # fc2 now takes input directly from conv2 output\nablated_model.eval()\n\n# Forward pass on ablated model\nwith torch.no_grad():\n    ablated_output = ablated_model(image.unsqueeze(0))\n    ablated_prediction = ablated_output.argmax(dim=1).item()\n\n# Print results\nprint(f\"True Label: {label}\")\nprint(f\"Original Model Prediction: {original_prediction}\")\nprint(f\"Ablated Model Prediction (fc1 removed): {ablated_prediction}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}