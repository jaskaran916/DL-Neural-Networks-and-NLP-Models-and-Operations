{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Lab Session: Implement a sequence-to-sequence model for machine translation using an encoder-decoder architecture.**","metadata":{"id":"IwHQTUkTGj5M"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n# Add any missing imports here, e.g.,\n# from IPython import get_ipython\n# from IPython.display import display\n\n# Example dataset class for parallel sentences\nclass TranslationDataset(Dataset):\n    def __init__(self, src_sentences, trg_sentences, src_vocab, trg_vocab, max_len=50):\n        self.src_sentences = src_sentences\n        self.trg_sentences = trg_sentences\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_sentences)\n\n    def __getitem__(self, idx):\n        src = self.tokenize(self.src_sentences[idx], self.src_vocab)\n        trg = self.tokenize(self.trg_sentences[idx], self.trg_vocab)\n        return torch.tensor(src), torch.tensor(trg)\n\n    def tokenize(self, sentence, vocab):\n        tokens = sentence.lower().strip().split()\n        idxs = [vocab.get(tok, vocab['<unk>']) for tok in tokens]\n        idxs = [vocab['<sos>']] + idxs + [vocab['<eos>']]\n        # pad\n        if len(idxs) < self.max_len:\n            idxs += [vocab['<pad>']] * (self.max_len - len(idxs))\n        else:\n            idxs = idxs[:self.max_len]\n        return idxs\n\n\n\n","metadata":{"id":"sLFPZRwkEYw3"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Encoder: bidirectional GRU\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.5):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim, hid_dim, n_layers,\n                          dropout=dropout, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n\n    def forward(self, src):\n        # src: [batch, src_len]\n        embedded = self.dropout(self.embedding(src))  # [batch, src_len, emb_dim]\n        outputs, hidden = self.gru(embedded)\n        # outputs: [batch, src_len, hid_dim*2]\n        # hidden: [n_layers*2, batch, hid_dim]\n        # Concatenate forward + backward hidden states\n        hidden = torch.tanh(self.fc(torch.cat(\n        (hidden[-2,:,:], hidden[-1,:,:]), dim=1)))  # [batch, hid_dim]\n        # Reshape to have n_layers\n        return outputs, hidden.unsqueeze(0).repeat(2, 1, 1)  # hidden for decoder init, with 2 layers\n","metadata":{"id":"JEILryBwG0F0"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Attention mechanism\nclass Attention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        # The input to the linear layer should be the sum of the hidden dimensions of the two inputs to torch.cat\n        self.attn = nn.Linear(hid_dim + hid_dim * 2, hid_dim)\n        self.v = nn.Linear(hid_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: [n_layers, batch, hid_dim]\n        # encoder_outputs: [batch, src_len, hid_dim*2]\n        batch_size = encoder_outputs.shape[0]\n        src_len = encoder_outputs.shape[1]\n\n        # hidden: [batch, n_layers, hid_dim] -> [batch, 1, n_layers, hid_dim]\n        hidden = hidden.transpose(0, 1).unsqueeze(1)\n\n        # hidden: [batch, src_len, n_layers, hid_dim]\n        hidden = hidden.repeat(1, src_len, 1, 1)\n\n        # encoder_outputs: [batch, src_len, hid_dim*2] -> [batch, src_len, 1, hid_dim*2]\n        encoder_outputs = encoder_outputs.unsqueeze(2)\n\n        # encoder_outputs: [batch, src_len, n_layers, hid_dim*2]\n        encoder_outputs = encoder_outputs.repeat(1, 1, hidden.shape[2], 1)\n\n        # Concatenate hidden and encoder_outputs along the last dimension\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=-1)))\n        # energy: [batch, src_len, n_layers, hid_dim]\n\n        attention = self.v(energy).squeeze(-1)  # attention: [batch, src_len, n_layers]\n\n        # Taking the average across n_layers, giving us attention weights for each word in the src sequence.\n        attention = attention.mean(dim=-1)  # attention: [batch, src_len]\n\n        return torch.softmax(attention, dim=1)","metadata":{"id":"SiD1lHkQG0cb"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n# Decoder: with attention\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=2, dropout=0.5):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.gru = nn.GRU(hid_dim*2 + emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(hid_dim*3 + emb_dim, output_dim)\n        self.attention = Attention(hid_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, encoder_outputs):\n        # input: [batch]\n        input = input.unsqueeze(1)  # [batch, 1]\n        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n        attn_weights = self.attention(hidden, encoder_outputs)  # [batch, src_len]\n        attn_weights = attn_weights.unsqueeze(1)  # [batch, 1, src_len]\n        context = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, hid_dim*2]\n        rnn_input = torch.cat((embedded, context), dim=2)\n        output, hidden = self.gru(rnn_input, hidden)\n        # output: [batch, 1, hid_dim]\n        embedded = embedded.squeeze(1)\n        output = output.squeeze(1)\n        context = context.squeeze(1)\n        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n        # prediction: [batch, output_dim]\n        return prediction, hidden, attn_weights.squeeze(1)\n","metadata":{"id":"acBeDoZIG0oV"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Seq2Seq wrapper\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.embedding.num_embeddings\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden = self.encoder(src)\n        input = trg[:,0]  # <sos>\n        for t in range(1, trg_len):\n            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n            outputs[:,t,:] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[:,t] if teacher_force else top1\n        return outputs\n","metadata":{"id":"1z_HSZGGHApA"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Usage example (training loop)\nif __name__ == \"__main__\":\n    # **Data Loading (Replace with your data loading logic)**\n    # Example:\n    # from utils import load_data, build_vocab\n    # src_sentences, trg_sentences = load_data(\"data.txt\")\n    # src_vocab = build_vocab(src_sentences)\n    # trg_vocab = build_vocab(trg_sentences)\n\n    # Placeholder (replace with actual data and vocabularies)\n    src_sentences = [\"I am fine .\", \"How are you ?\"]\n    trg_sentences = [\"Je vais bien .\", \"Comment allez-vous ?\"]\n    src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'i': 4, 'am': 5, 'fine': 6, '.': 7, 'how': 8, 'are': 9, 'you': 10, '?': 11}\n    trg_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'je': 4, 'vais': 5, 'bien': 6, '.': 7, 'comment': 8, 'allez-vous': 9, '?': 10}\n\n    dataset = TranslationDataset(src_sentences, trg_sentences, src_vocab, trg_vocab)\n    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    encoder = Encoder(input_dim=len(src_vocab), emb_dim=256, hid_dim=512).to(device)\n    decoder = Decoder(output_dim=len(trg_vocab), emb_dim=256, hid_dim=512).to(device)\n    model = Seq2Seq(encoder, decoder, device).to(device)\n\n    optimizer = optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab['<pad>'])\n\n    model.train()","metadata":{"id":"56s1l457HAwf"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n    for epoch in range(1, 11):  # Example: Train for 10 epochs\n        epoch_loss = 0\n        for src_batch, trg_batch in loader:\n            src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n            optimizer.zero_grad()\n            output = model(src_batch, trg_batch)\n            # reshape: [(batch*trg_len), vocab]\n            output_dim = output.shape[-1]\n            output = output[:,1:,:].reshape(-1, output_dim)\n            trg = trg_batch[:,1:].reshape(-1)\n            loss = criterion(output, trg)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n            optimizer.step()\n            epoch_loss += loss.item()\n        print(f\"Epoch {epoch} Loss: {epoch_loss/len(loader):.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCMgqy_gHA0i","outputId":"760f6c4a-679d-4d0f-9684-56fbf5fd58f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Loss: 2.3255\n","Epoch 2 Loss: 2.2261\n","Epoch 3 Loss: 1.9156\n","Epoch 4 Loss: 1.4333\n","Epoch 5 Loss: 1.2246\n","Epoch 6 Loss: 0.8954\n","Epoch 7 Loss: 0.6293\n","Epoch 8 Loss: 0.4267\n","Epoch 9 Loss: 0.2713\n","Epoch 10 Loss: 0.2876\n"]}],"execution_count":13}]}