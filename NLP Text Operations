{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaskarandhillon1609/nlp-text-operations?scriptVersionId=217733364\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"8bb638bb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-15T07:04:12.35032Z","iopub.status.busy":"2025-01-15T07:04:12.349812Z","iopub.status.idle":"2025-01-15T07:04:13.441365Z","shell.execute_reply":"2025-01-15T07:04:13.440233Z"},"papermill":{"duration":1.097308,"end_time":"2025-01-15T07:04:13.443687","exception":false,"start_time":"2025-01-15T07:04:12.346379","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"739a913f","metadata":{"execution":{"iopub.execute_input":"2025-01-15T07:04:13.448704Z","iopub.status.busy":"2025-01-15T07:04:13.448203Z","iopub.status.idle":"2025-01-15T07:04:16.425101Z","shell.execute_reply":"2025-01-15T07:04:16.423614Z"},"papermill":{"duration":2.981997,"end_time":"2025-01-15T07:04:16.427721","exception":false,"start_time":"2025-01-15T07:04:13.445724","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Sentences:\n","['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.']\n","\n","Word Count:\n","66\n"]}],"source":["import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","# Ensure that the punkt resource is downloaded\n","nltk.download('punkt')\n","\n","# Define the text to be tokenized\n","text = (\n","    \"NLTK is a leading platform for building Python programs to work with human language data. \"\n","    \"It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, \"\n","    \"along with a suite of text processing libraries for classification, tokenization, stemming, \"\n","    \"tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, \"\n","    \"and an active discussion forum.\"\n",")\n","\n","# Function to tokenize sentences\n","def tokenize_sent(text):\n","    sentences = sent_tokenize(text)\n","    return sentences\n","\n","# Tokenize the text into sentences\n","sentences = tokenize_sent(text)\n","\n","# Print the list of sentences\n","print(\"Sentences:\")\n","print(sentences)\n","\n","# Function to tokenize words\n","def tokenize_words(text):\n","    words = word_tokenize(text)\n","    return words\n","\n","# Tokenize the text into words\n","words = tokenize_words(text)\n","\n","# Count the number of words\n","word_count = len(words)\n","\n","# Print the word count\n","print(\"\\nWord Count:\")\n","print(word_count)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":7.918818,"end_time":"2025-01-15T07:04:17.251266","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-15T07:04:09.332448","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}