{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2ea60498-7972-4cc1-a043-d11fbe4ac886","cell_type":"markdown","source":"1. Pre-processing using NLTK ","metadata":{}},{"id":"47cdb0dd-bdcc-44f6-8a39-d3871b0914e0","cell_type":"code","source":"import nltk \nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer \nimport string \n# Download necessary NLTK resources \nnltk.download('punkt') \nnltk.download('stopwords') \nnltk.download('wordnet') \ndef preprocess_text(text): \n# Lower casing \n    text = text.lower() \n# Tokenization \n    tokens = nltk.word_tokenize(text) \n# Remove punctuation \n    tokens = [word for word in tokens if word.isalnum()] \n# Remove stop words \n    tokens = [word for word in tokens if word not in stopwords.words('english')] \n# Lemmatization \n    lemmatizer = WordNetLemmatizer() \n    tokens = [lemmatizer.lemmatize(word) for word in tokens] \n# Stemming \n    stemmer = PorterStemmer() \n    tokens = [stemmer.stem(word) for word in tokens] \n    return tokens \n# Example usage \ntext = \"NLTK is a leading platform for building Python programs to work with human language data.\" \nprocessed_text = preprocess_text(text) \nprint(processed_text)","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["['nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data']\n"]}],"execution_count":11},{"id":"13d1598e-94a3-4587-b142-7b6efa3eb34d","cell_type":"markdown","source":"2. N-grams Generation ","metadata":{}},{"id":"eb4d5945-1781-445f-8d0d-f11611ed5577","cell_type":"code","source":"from nltk import ngrams \ndef generate_ngrams(text, n): \n    tokens = nltk.word_tokenize(text) \n    return list(ngrams(tokens, n)) \n# Example usage \ntext = \"I love natural language processing\"\nbigrams = generate_ngrams(text, 2) \nprint(bigrams)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]\n"]}],"execution_count":31},{"id":"a4e44ede-33b0-4d06-80e7-05963b800c82","cell_type":"markdown","source":"3. Synonyms and Antonyms Identification ","metadata":{}},{"id":"dea7945e-46d8-491a-aba0-6abf52712de1","cell_type":"code","source":"from nltk.corpus import wordnet \n \nnltk.download('wordnet') \n \ndef get_synonyms_antonyms(word): \n    synonyms = [] \n    antonyms = [] \n    for syn in wordnet.synsets(word): \n        for lemma in syn.lemmas(): \n            synonyms.append(lemma.name()) \n            if lemma.antonyms(): \n                antonyms.append(lemma.antonyms()[0].name()) \n    return set(synonyms), set(antonyms) \n \n# Example usage \nsynonyms, antonyms = get_synonyms_antonyms(\"happy\") \nprint(\"Synonyms:\", synonyms) \nprint(\"Antonyms:\", antonyms)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Synonyms: {'felicitous', 'glad', 'happy', 'well-chosen'}\n","Antonyms: {'unhappy'}\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"execution_count":36},{"id":"7c198900-8ec6-4916-b117-c6023ba91f5e","cell_type":"markdown","source":"4. TF-IDF Implementation","metadata":{}},{"id":"b8538938-9910-428f-be24-b4e2c40454b0","cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \n\n# Sample corpus \ncorpus = [ \n    \"This is the first document.\", \n    \"This document is the second document.\", \n    \"And this is the third one.\", \n    \"Is this the first document?\" \n] \n\ndef compute_tfidf(corpus): \n    vectorizer = TfidfVectorizer()  # Corrected capitalization\n    tfidf_matrix = vectorizer.fit_transform(corpus) \n    return tfidf_matrix, vectorizer.get_feature_names_out() \n\n# Example usage \ntfidf_matrix, feature_names = compute_tfidf(corpus) \nprint(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray()) \nprint(\"Feature Names:\", feature_names)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF Matrix:\n"," [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]\n"," [0.         0.6876236  0.         0.28108867 0.         0.53864762\n","  0.28108867 0.         0.28108867]\n"," [0.51184851 0.         0.         0.26710379 0.51184851 0.\n","  0.26710379 0.51184851 0.26710379]\n"," [0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n","Feature Names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"]}],"execution_count":57},{"id":"13a00b33-c908-48e9-b32e-37780d428e67","cell_type":"markdown","source":"5. Part-of-Speech (PoS) Tagging","metadata":{}},{"id":"475c71e7-140f-4f23-9750-5d0ab3b7ca22","cell_type":"code","source":"import nltk\n\n# Download the required NLTK data\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')  # Also download the punkt tokenizer if not already downloaded\n\ndef pos_tagging(text): \n    tokens = nltk.word_tokenize(text) \n    return nltk.pos_tag(tokens)\n\n# Example usage \ntext = \"Python is an amazing programming language.\" \npos_tags = pos_tagging(text) \nprint(pos_tags)","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n"]},{"name":"stdout","output_type":"stream","text":["[('Python', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('amazing', 'JJ'), ('programming', 'NN'), ('language', 'NN'), ('.', '.')]\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"execution_count":64},{"id":"e57802a6-8276-4d3f-bc62-de69f651ea0a","cell_type":"markdown","source":"6. Named Entity Recognition (NER) ","metadata":{}},{"id":"d9854b51-4a91-4a4b-8bdf-46a0de3f264e","cell_type":"code","source":"import nltk\n\n# Download the required NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\ndef named_entity_recognition(text): \n    tokens = nltk.word_tokenize(text)  \n    tagged = nltk.pos_tag(tokens)       \n    return nltk.ne_chunk(tagged)        \n\n# Example usage \ntext = \"Barack Obama was the 44th President of the United States.\"  \nner_tree = named_entity_recognition(text) \nprint(ner_tree)  ","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(S\n","  (PERSON Barack/NNP)\n","  (PERSON Obama/NNP)\n","  was/VBD\n","  the/DT\n","  44th/JJ\n","  President/NNP\n","  of/IN\n","  the/DT\n","  (GPE United/NNP States/NNPS)\n","  ./.)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to\n","[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}],"execution_count":71},{"id":"b503eddf-2eee-411a-8efa-f20bce57f56a","cell_type":"markdown","source":"7. Sentiment Analysis ","metadata":{}},{"id":"9b43f5a8-a522-49f3-853a-5db6e9ab21d3","cell_type":"code","source":"\nfrom textblob import TextBlob \n\ndef sentiment_analysis(text): \n    analysis = TextBlob(text) \n    return analysis.sentiment.polarity  # Returns a value between -1 and 1 \n\n# Example usage \ntext = \"I love programming in Python!\" \nsentiment_score = sentiment_analysis(text) \nprint(\"Sentiment Score:\", sentiment_score)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentiment Score: 0.625\n"]}],"execution_count":78},{"id":"31035e92-4630-48b3-ad80-849b034b7412","cell_type":"markdown","source":"8. Spam Filter Development","metadata":{}},{"id":"16fe0fe9-7a20-4a92-bd96-6b035cbee59b","cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.pipeline import make_pipeline \nfrom sklearn.model_selection import train_test_split \n\n# Sample dataset \ndata = [  # Changed 'Data' to 'data' to follow naming conventions\n    (\"Free money now!!!\", 1), \n    (\"Hi, how are you?\", 0), \n    (\"Get paid to work from home.\", 1), \n    (\"Hello, I wanted to check in.\", 0), \n] \n\n# Split data into features and labels \nX, y = zip(*data) \n\n# Train-test split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) \n\ndef train_spam_filter(X_train, y_train):  # Changed 'Def' to 'def'\n    model = make_pipeline(CountVectorizer(), MultinomialNB())  # Changed 'Model' to 'model'\n    model.fit(X_train, y_train) \n    return model  # Changed 'Return' to 'return'\n\n# Train the model \nspam_model = train_spam_filter(X_train, y_train)  # Changed 'Spam_model' to 'spam_model'\n\n# Example usage \ntest_message = [\"Congratulations! You’ve won a free ticket!\", \"Can we meet tomorrow?\"]  # Changed quotes\npredictions = spam_model.predict(test_message)  # Changed 'Predictions' to 'predictions'\nprint(\"Predictions (1: Spam, 0: Not Spam):\", predictions)  # Changed 'Print' to 'print'","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions (1: Spam, 0: Not Spam): [1 1]\n"]}],"execution_count":81},{"id":"23f19b03-830d-41b7-a788-6a3390015157","cell_type":"markdown","source":"9. Fake News Detection ","metadata":{}},{"id":"e6239a5f-c757-45aa-8f28-4c8b248c43ae","cell_type":"code","source":"import pandas as pd \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.pipeline import make_pipeline \n\n# Sample dataset (you can replace this with a real dataset) \ndata = { \n    'text': [ \n        \"Breaking: New study shows that eating chocolate can help you lose weight.\", \n        \"Local man wins lottery and donates to charity.\", \n        \"Scientists discover a new planet that could support life.\", \n        \"New study reveals that drinking coffee can lead to heart disease.\", \n        \"The moon landing was staged.\" \n    ], \n    'label': [1, 0, 1, 0, 0]  # 1: Fake, 0: Real \n} \n\ndf = pd.DataFrame(data) \n\n# Split data into features and labels \nX = df['text'] \ny = df['label'] \n\n# Train-test split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) \n\ndef train_fake_news_detector(X_train, y_train): \n    model = make_pipeline(TfidfVectorizer(), LogisticRegression())  # Indented correctly\n    model.fit(X_train, y_train) \n    return model \n\n# Train the model \nfake_news_model = train_fake_news_detector(X_train, y_train) \n\n# Example usage \ntest_articles = [ \n    \"New evidence suggests that climate change is a hoax.\", \n    \"Local community comes together to support homeless shelter.\" \n] \n\npredictions = fake_news_model.predict(test_articles) \nprint(\"Predictions (1: Fake News, 0: Real News):\", predictions)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions (1: Fake News, 0: Real News): [0 0]\n"]}],"execution_count":84}]}